{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger('tpot_log')\n",
    "\n",
    "# Set the log level\n",
    "logger.setLevel(logging.INFO)  # Adjust the level as needed (INFO, DEBUG, etc.)\n",
    "\n",
    "# Define a log file to save the log information\n",
    "log_file = 'tpot_log.txt'\n",
    "\n",
    "# Create a file handler and add it to the logger\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Optionally, create a console handler to log messages to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Define the log format\n",
    "log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "formatter = logging.Formatter(log_format)\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Example: Log a message\n",
    "logger.info('Logging has been set up for TPOT.')\n",
    "\n",
    "# Now, TPOT's log messages will be written to the specified log file and optionally shown in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd093382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from array import array\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "temp = dict(layout = go.Layout(font = dict(family=\"Franklin Gothic\", size=12), width = 1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b88097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847e701",
   "metadata": {},
   "source": [
    "## TPOT Dictionary for Feature Processing and ML Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "regressor_config_dict={'sklearn.linear_model.ElasticNetCV': {\n",
    "        'l1_ratio': [0.05, 0.1, 0.5, 1.0],\n",
    "        'tol': [1e-05, 0.001, 0.1]\n",
    "    },\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'loss': ['ls', 'huber'],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.1, 0.5, 1.0],\n",
    "        'max_features': [0.1, 0.5, 1.0],\n",
    "    },\n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_features': [0.1, 0.5, 1.0],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    },\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'subsample': [0.1, 0.5, 1.0],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "    },\n",
    "    'sklearn.svm.LinearSVR': {\n",
    "        'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-05, 0.001, 0.1],\n",
    "        'C': [0.001, 0.1, 1.0, 10.0, 25.0],\n",
    "        'epsilon': [0.001, 0.1, 1.0],\n",
    "    },\n",
    "    'sklearn.ensemble.AdaBoostRegressor': {\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "    },\n",
    "\n",
    "\n",
    "'sklearn.impute.SimpleImputer': {'strategy':'mean'},\n",
    " 'sklearn.impute.SimpleImputer': {'strategy':'median'},\n",
    " 'sklearn.impute.KNNImputer': {},\n",
    " 'sklearn.preprocessing.MaxAbsScaler': {},\n",
    " 'sklearn.preprocessing.MinMaxScaler': {},\n",
    " 'sklearn.preprocessing.Normalizer': {'norm': ['l1', 'l2', 'max']},\n",
    "     'sklearn.kernel_approximation.Nystroem': {\n",
    "    'kernel': ['rbf', 'polynomial', 'linear'],\n",
    "    'gamma': [0.1, 0.5, 1.0],\n",
    "    'n_components': range(1, 6)  # Reduced the range for n_components\n",
    "},\n",
    "            'sklearn.decomposition.PCA': {'svd_solver': ['randomized'],\n",
    "  'iterated_power': range(1, 11)},\n",
    "                'sklearn.kernel_approximation.RBFSampler': {'gamma': [0.  , 0.2 , 0.35,  0.45, 0.5 ,\n",
    "         0.55, 0.6 , 0.65, 0.8 , 1.  ]},\n",
    "                    'sklearn.preprocessing.RobustScaler': {},\n",
    " 'sklearn.preprocessing.StandardScaler': {},\n",
    " 'sklearn.preprocessing.QuantileTransformer': {},\n",
    " 'sklearn.preprocessing.PowerTransformer': {},\n",
    "    \n",
    "     'sklearn.feature_selection.SelectFwe': {'alpha': [0.01, 0.05, 0.1,0.15, 0.2,0.25, 0.3,0.35,0.4,0.45,0.5],\n",
    "  'score_func': {'sklearn.feature_selection.f_regression': None}},\n",
    "            \n",
    "             'sklearn.feature_selection.SelectPercentile': {'percentile': range(1, 100,20),\n",
    "  'score_func': {'sklearn.feature_selection.f_regression': None}},\n",
    " 'sklearn.feature_selection.VarianceThreshold': {'threshold': [\n",
    "   0.001,\n",
    "   0.01,\n",
    "   0.1,\n",
    "   0.2]},\n",
    "     'sklearn.feature_selection.SelectFromModel': {'threshold': [0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
    "         0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ],\n",
    "  'estimator': {'sklearn.ensemble.ExtraTreesRegressor': {'n_estimators': [100],\n",
    "    'max_features': [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
    "           0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4c588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsi(df, period):\n",
    "\n",
    "    delta = df['wap'].pct_change()\n",
    "\n",
    "\n",
    "    # Calculate the average gain and average loss for the specified period\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(period).mean()\n",
    "    avg_loss = loss.rolling(period).mean()\n",
    "\n",
    "    # Calculate the Relative Strength (RS) by dividing the average gain by the average loss\n",
    "    rs = avg_gain / avg_loss\n",
    "\n",
    "    # Calculate the Relative Strength Index (RSI)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi \n",
    "\n",
    "def RSI_Signal(rsi):\n",
    "\n",
    "    if rsi > 70: # Overbought\n",
    "        return -1\n",
    "    \n",
    "    elif rsi < 30: # Oversold\n",
    "        return 1\n",
    "    \n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def obv_volume(delta, volume):\n",
    "\n",
    "    if delta > 0 :\n",
    "        return volume\n",
    "    elif delta < 0 :\n",
    "        return -volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb17694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "def generate_features(df,preprocess):\n",
    "    features = ['seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "               'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
    "                'imb_s1', 'imb_s2'\n",
    "               ]\n",
    "    \n",
    "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)')\n",
    "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)')\n",
    "    \n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            if i>j:\n",
    "                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})')\n",
    "                features.append(f'{a}_{b}_imb')    \n",
    "                    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            for k,c in enumerate(prices):\n",
    "                if i>j and j>k:\n",
    "                    max_ = df[[a,b,c]].max(axis=1)\n",
    "                    min_ = df[[a,b,c]].min(axis=1)\n",
    "                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_\n",
    "\n",
    "                    df[f'{a}_{b}_{c}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "                    features.append(f'{a}_{b}_{c}_imb2')\n",
    "    \n",
    "    \n",
    "    train_df_1 = pd.DataFrame()\n",
    "\n",
    "    for name, single_stock_train_df in tqdm(df.groupby('stock_id')):\n",
    "\n",
    "        single_stock_train_df['rsi'] = get_rsi(single_stock_train_df, period = 14)\n",
    "        single_stock_train_df['rsi signal'] = single_stock_train_df.apply(lambda x: RSI_Signal(x['rsi']), axis = 1)\n",
    "        train_df_1 = pd.concat([train_df_1, single_stock_train_df], axis = 0)\n",
    "\n",
    "    features.append('rsi')\n",
    "    features.append('rsi signal')\n",
    "    \n",
    "    \n",
    "    train_df_2 = pd.DataFrame()\n",
    "\n",
    "    for name, single_stock_train_df in tqdm(train_df_1.groupby('stock_id')):\n",
    "\n",
    "        single_stock_train_df['delta(wap)'] = single_stock_train_df['wap'].pct_change()\n",
    "        single_stock_train_df['Volume Adjust'] = single_stock_train_df.apply(lambda x: obv_volume(x['delta(wap)'], x['matched_size']), axis = 1)\n",
    "        single_stock_train_df['OBV'] = single_stock_train_df['Volume Adjust'].rolling(window = 14).sum()\n",
    "\n",
    "        train_df_2 = pd.concat([train_df_2, single_stock_train_df], axis = 0)\n",
    "    \n",
    "    features.append('delta(wap)')\n",
    "    features.append('Volume Adjust')\n",
    "    features.append('OBV')\n",
    "    \n",
    "    train_df_2['Imbalance'] = train_df_2['imbalance_buy_sell_flag'] * train_df_2['imbalance_size']\n",
    "    train_df_2['Bid/Ask Spread'] = train_df_2['bid_price'] - train_df_2['ask_price']\n",
    "    \n",
    "    features.append('Imbalance')\n",
    "    features.append('Bid/Ask Spread')\n",
    "    \n",
    "    if preprocess:\n",
    "        default_value = -99999999\n",
    "        train_df_2.fillna(default_value, inplace=True)\n",
    "    \n",
    "    \n",
    "        # Replace Inf with NaN\n",
    "        train_df_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Replace NaN with a default value (e.g., 0)\n",
    "        default_value = -99999998\n",
    "        train_df_2.fillna(default_value, inplace=True)\n",
    "    \n",
    "    train_df_2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return train_df_2[features]\n",
    "\n",
    "TRAINING = True\n",
    "if TRAINING:\n",
    "    ## Have to edit this to use a sampling code, sample close to 1 MM  \n",
    "    df_train = pd.read_csv('train.csv').head(500000)\n",
    "    df_train.dropna(subset=\"target\",inplace=True)\n",
    "    df_ = generate_features(df_train,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a51b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45467884",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fbccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39840fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86d21f4",
   "metadata": {},
   "source": [
    "### The following took 1 day to run, use the next tab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a14599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the TPOTRegressor with the specified 'config_dict'\n",
    "tpot2 = TPOTRegressor(generations=20, \n",
    "                    population_size=80, cv=5, verbosity=3, random_state=42, config_dict=regressor_config_dict,\n",
    "                      max_time_mins=600,periodic_checkpoint_folder=\"intermediate_results\",max_eval_time_mins=30,\n",
    "                    n_jobs=-1,log_file='tpot_log.txt')\n",
    "tpot2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b08d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the TPOTRegressor with the specified 'config_dict'\n",
    "tpot = TPOTRegressor(generations=20, \n",
    "                    population_size=80, cv=5, verbosity=3, random_state=42, config_dict=regressor_config_dict,max_time_mins=30   ,periodic_checkpoint_folder=\"intermediate_results\",\n",
    "                    n_jobs=-1)#,early_stop=8)\n",
    "tpot.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64ee25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Access the best pipeline\n",
    "best_pipeline = tpot.fitted_pipeline_\n",
    "\n",
    "# Save the best pipeline to a file\n",
    "joblib.dump(best_pipeline, 'best_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75ddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a213e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "training_features = imputer.transform(X_train)\n",
    "testing_features = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a661924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved pipeline\n",
    "loaded_pipeline = joblib.load('best_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c0c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(tpot.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62954306",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(loaded_pipeline.predict(testing_features),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
